# Dense vs. Sparse Baselines for Legal Case Retrieval (COLIEE 2025 Task 1)

This repository contains the implementation of a comparative study between Sparse (BM25) and Dense (BERT-based) Information Retrieval systems applied to the Legal Case Retrieval (LCR) task.

This project was developed as benchmarks peformance through the **COLIEE 2025 Task 1** challenge: finding relevant prior cases that support or conflict with a given query case.

## üìÑ Abstract & Key Findings

Legal retrieval differs from general open-domain QA because relevance often hinges on precise lexical markers (e.g., specific statute numbers, "onus of proof") rather than broad semantic similarity. This project implements and evaluates:

1.  **Sparse Baseline:** BM25 (Probabilistic retrieval).
2.  **Dense Retrieval:** BERT-base vs. LegalBERT (Domain-adapted).
3.  **Optimization:** Impact of Chunking, Pooling strategies (CLS vs. Mean), and Dimensionality Reduction (PCA).

### Key Results
Based on experiments run on the official COLIEE dataset:
* **The "Lexical Gap":** Sparse BM25 (`MRR 0.233`) significantly outperformed the best Dense configuration (`MRR 0.178`), proving that "naive" dense models struggle with fine-grained legal details.
* **Domain Adaptation:** LegalBERT outperformed BERT-base by ~22% in MRR, validating that domain-specific pre-training captures legal sub-language better.
* **PCA Denoising:** Reducing embedding dimensions from 768 to 128 actually **improved** retrieval performance (MRR 0.143 ‚Üí 0.148), suggesting that PCA helps remove syntactic noise from the latent space.

---

## ‚ö†Ô∏è Data Disclaimer (Important)

**The `.txt` files currently in the `data/` folder are synthetic/dummy data generated by LLMs.**

Due to the terms of use for the COLIEE competition, I am not permitted to redistribute the actual Federal Court of Canada case law corpus.

### How to reproduce with real data:
1.  Go to the [COLIEE Website](https://coliee.org).
2.  Fill out the dataset request forms to obtain the Task 1 Corpus.
3.  Replace the files in `data/train_files` and `data/test_files` with the official `.txt` files.
4.  Update `train_labels.json` with the official ground truth.

---

## ‚ö° Computational Requirements

**Note:** The dense retrieval pipeline (specifically the embedding generation for thousands of long legal documents) is computationally intensive.

* **Development:** Code structure was developed and tested locally.
* **Production Runs:** The actual experiments and embedding generation were executed on **Kaggle TPUs** to handle the high throughput required for BERT tokenization and inference.
* **Recommendation:** If running this locally on the full dataset, ensure you have a CUDA-enabled GPU. CPU-only execution will be extremely slow.


## Methodology

### 1. Preprocessing
Raw legal cases are noisy. We utilize aggressive regex cleaning to remove:
* Boilerplate headers/footers
* Fragment markers (`<FRAGMENT_SUPPRESSED>`)
* Page numbers and editor notes
* Non-substantive punctuation lines

### 2. Retrieval Architectures

| Approach | Model | Description |
| :--- | :--- | :--- |
| **Sparse** | **BM25** | Uses term frequency and inverse document frequency. Captures exact keyword matches effectively. |
| **Dense** | **BERT-base** | General-purpose transformer (Wikipedia pre-trained). |
| **Dense** | **LegalBERT** | Pre-trained on 12GB of legal text. Better at polysemous legal terms (e.g., "consideration"). |

### 3. Handling Long Documents
Since legal cases exceed the 512-token BERT limit, we employ **Sliding Window Chunking**:
* **Stride:** 256 or 512 tokens.
* **Pooling:** * `tokens`: Mean pooling of all word embeddings (Found to be superior).
    * `cls_only`: Using the specialized [CLS] summary token.
* **Aggregation:** Chunk embeddings are averaged to create a single Document Vector.

### 4. Dimensionality Reduction
We use **PCA** (Principal Component Analysis) to project 768-dim embeddings down to 348 and 128 dimensions. This utilizes **FAISS** for efficient vector storage and similarity search.

---

### Running the Pipeline
You can run the full pipeline via the `notebooks/experiments.ipynb` or execute modular components via Python:

```python
from src.preprocess import clean_all_datasets
from src.bm25 import output_bm25_rankings
from src.dense_embeddings_helpers import embed_dataset
from src.evaluate_rankings import evaluate_all_rankings

# 1. Clean Data (Regex)
clean_all_datasets()

# 2. Run Sparse Baseline
output_bm25_rankings(train=True)

# 3. Generate Dense Embeddings (Requires GPU/TPU recommended)
# Options: model="LegalBERT" or "bert-base-uncased"
#          pool_strategy="tokens" or "cls_only"
embed_dataset(model="LegalBERT", pool_strategy="tokens", stride=256, train=True)

# 4. Generate Rankings from Embeddings
# Note: This example uses the default output path from step 3
from src.dense_rankings_helpers import output_embedding_rankings
from src.config import EMBEDDINGS_DIR
embedding_file = EMBEDDINGS_DIR / 'train_LegalBERT_tokens_stride=256_dim=768.npz'
output_embedding_rankings(embedding_file, train=True)

# 5. Evaluate Results
results = evaluate_all_rankings(train=True)
print(results)
```

## License

MIT License
